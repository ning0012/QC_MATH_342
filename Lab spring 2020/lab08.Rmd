---
title: "Lab 8"
author: "Jianing Guo"
output: pdf_document
date: "2AM April 26, 2020"
---

# Data Wrangling / Munging / Carpentry

Throughout this assignment you can use either `dplyr` or `data.table` to answer but not base R.

Load the `storms` dataset from the `dplyr` package and investigate it using `str` and `summary` and `head`. Which two columns should be converted to type factor? Do so below.

```{r}
pacman::p_load(dplyr, magrittr)
data("storms")
str(storms)
summary(storms)
head(storms)
storms %<>% 
  mutate(name = factor(name), status = factor(status))
str(storms)
```

Reorder the columns so name is first, status is second, category is third and the rest are the same.

```{r}
storms %<>% 
  select(name, status, category, everything())
storms
```

Find a subset of the data of storms only in the 1970's.

```{r echo = T, results = 'hide'}
storms %>% 
  filter(year %in% 1970:1979)
```

Find a subset of the data of storm observations only with category 4 and above and wind speed 100MPH and above.

```{r echo = T, results = 'hide' }
storms %>% 
  filter(category >= 4, wind >= 100)
```

Create a new feature `wind_speed_per_unit_pressure`.

```{r}
storms %<>%
  mutate(wind_speed_per_unit_pressure = wind / pressure)
storms
```

Create a new feature: `average_diameter` which averages the two diameter metrics. If one is missing, then use the value of the one that is present. If both are missing, leave missing.

```{r}
storms %<>% 
  mutate(average_diameter = ifelse(!is.na(ts_diameter) & !is.na(hu_diameter), (ts_diameter + hu_diameter) / 2,
                                   ifelse(is.na(ts_diameter), hu_diameter,
                                          ifelse(is.na(hu_diameter), ts_diameter, NA))))
storms
```


For each storm, summarize the maximum wind speed. "Summarize" means create a new dataframe with only the summary metrics you care about.

```{r}
#TO-DO
```

Order your dataset by maximum wind speed storm but within the rows of storm show the observations in time order from early to late.

```{r  echo = T, results = 'hide'}
storms %>% 
  group_by(name, year) %>% 
  summarise(max_wind_speed = max(wind)) %>% 
  arrange(-max_wind_speed, year)
```

Find the strongest storm by wind speed per year.

```{r}
storms %>% 
  group_by(year) %>% 
  summarize(name = first(name), max_wind = max(wind)) %>% 
  arrange(-max_wind)
```

For each named storm, find its maximum category, wind speed, pressure and diameters. Do not allow the max to be NA (unless all the measurements for that storm were NA).

```{r echo = T, results = 'hide'}
storms %>%
  group_by(name) %>%
  summarise(max_category = max(category),
            max_wind_speed = max(wind),
            max_pressure = max(pressure),
            max_ts_diam = max(ts_diameter),
            max_hu_diam = max(hu_diameter)) %>% 
  na.omit()
```


For each year in the dataset, tally the number of storms. "Tally" is a fancy word for "count the number of". Plot the number of storms by year. Any pattern?

```{r}
pacman::p_load(ggplot2)
storms %>% 
  group_by(year) %>% 
  summarise(n_storms = n()) %>% 
  ggplot() + geom_point(aes(x = n_storms, y = year))
```

For each year in the dataset, tally the storms by category.

```{r echo = T, results = 'hide'}
storms %>% 
  group_by(year, category) %>% 
  summarise(n_storms = n())
```

For each year in the dataset, find the maximum wind speed per status level.

```{r}
storms %>% 
  group_by(year, status) %>% 
  summarize(max_wind_speed = max(wind))
```

For each storm, summarize its average location in latitude / longitude coordinates.

```{r}
storms %<>% 
  group_by(name) %>% 
  summarise(avg_lat = mean(lat), avg_long = mean(long))
```

For each storm, summarize its duration in number of hours (to the nearest 6hr increment).

```{r}
storms %>% 
  group_by(name) %>% 
  summarise(duration = ifelse(sum(hour) %% 6 == 0, sum(hour), 
                              ifelse(sum(hour) %% 6 < 3, sum(hour) - (sum(hour) %% 6), 
                                     sum(hour) + (6-(sum(hour) %% 6)))))
  
```

Convert year, month, day, hour into the variable `timestamp` using the `lubridate` package.

```{r}
pacman::p_load(lubridate)
storms %<>% 
  mutate(timestamp = ymd_h(paste(year, month, day, hour, sep = "-"))) %>% 
  select(-year, -month, -day, -hour)
```

Using the `lubridate` package, create new variables `day_of_week` which is a factor with levels "Sunday", "Monday", ... "Saturday" and `week_of_year` which is integer 1, 2, ..., 52.

```{r}
storms %<>%
  mutate(day_of_week = wday(timestamp, label = TRUE)) %<>%
  mutate(week_of_the_year = week(timestamp))
```

For each storm, summarize the day in which is started in the following format "Friday, June 27, 1975".

```{r}
#TO-DO
```

Create a new factor variable `decile_windspeed` by binning wind speed into 10 bins and drop `wind`.

```{r}
bins <- 0:10
storms %<>% 
  mutate(decile_windspeed = factor(cut(wind, breaks = quantile(wind, bins/10), labels = FALSE)))
```

Create a new data frame `serious_storms` which are category 3 and above hurricanes.

```{r}
serious_storms = storms %>% 
  filter(category >= 3)
serious_storms
```

In `serious_storms`, merge the variables lat and long together into `lat_long` with values `lat / long` as a string.

```{r}
serious_storms %<>%
  mutate(lat_long = paste(lat, long, sep = " / ")) %>% 
  select(-lat, -long)
```

For each category, find the average wind speed, pressure and diameters (do not count the NA's in your averaging).

```{r echo = T, results = 'hide'}
storms %>% 
  group_by(category) %>% 
  summarise(avgrage_wind_speed = mean(wind),
            avgrage_pressure = mean(pressure),
            avgrage_ts_diam = mean(ts_diameter, na.rm = TRUE),
            avgrage_hu_diam = mean(hu_diameter, na.rm = TRUE))
```

For each named storm, find its maximum category, wind speed, pressure and diameters (do not allow the max to be NA) and the number of readings (i.e. observations).

```{r}
#TO-DO
```

Calculate the distance from each storm observation to Miami in a new variable `distance_to_miami`. This is very challenging. You will need a function that computes distances from two sets of latitude / longitude coordinates. 

```{r}
MIAMI_COORDS = c(25.7617, -80.1918)
#TO-DO
```

For each storm observation, use the function from the previous question to calculate the distance it moved since the previous observation.

```{r}
#TO-DO
```

For each storm, find the total distance it moved over its observations and its total displacement. "Distance" is a scalar quantity that refers to "how much ground an object has covered" during its motion. "Displacement" is a vector quantity that refers to "how far out of place an object is"; it is the object's overall change in position.

```{r}
#TO-DO
```

For each storm observation, calculate the average speed the storm moved in location.

```{r}
#TO-DO
```

For each storm, calculate its average ground speed (how fast its eye is moving which is different from windspeed around the eye).

```{r}
#TO-DO
```

Is there a relationship between average ground speed and maximum category attained? Use a dataframe summary (not a regression).

```{r}
#TO-DO
```

Now we want to transition to building real design matrices for prediction. This is more in tune with what happens in the real world. Large data dump and you convert it into $X$ and $y$ how you see fit.

Suppose we wish to predict the following: given the first three readings of a storm, can you predict its maximum wind speed? Identify the `y` and identify which features you need $x_1, ... x_p$ and build that matrix with `dplyr` functions. This is not easy, but it is what it's all about. Feel free to "featurize" as creatively as you would like. You aren't going to overfit if you only build a few features relative to the total 198 storms.

```{r}
#TO-DO
```

Fit your model. Validate it. Assess your level of success at this endeavor.
 
```{r}

```


# Interactions in linear models

Load the Boston Housing Data from package `MASS` and use `str` and `summary` to remind yourself of the features and their types and then use `?MASS::Boston` to read an English description of the features.

```{r}
data(Boston, package = "MASS")
str(Boston)
summary(Boston)
?MASS::Boston
```



Using what you learned about the Boston Housing Data in the previous question, try to guess which features are interacting. Confirm using plots in `ggplot` that illustrate three (or more) features.

```{r}
ggplot(data = Boston) +
  geom_point(aes(x = rm, y = medv, color = tax))
```

Once an interaction has been located, confirm the "non-linear linear" model with the interaction term does better than just the vanilla linear model by demonstrating a lower RMSE. In Econ 382 you would test this explicitly using a hypothesis test. We know in this class than increasing $p$ yields alower RMSE. But the exercise is still a good one.

```{r}
mod_linear = lm(medv ~ ., data = Boston)
Mod_interaction1 = lm(medv ~ . + (rm * tax), data = Boston)
summary(mod_linear)$sigma
summary(Mod_interaction1)$sigma
```

Repeat this procedure for another interaction with two different features (not used in the previous interaction you found) and verify.

```{r}
Mod_interaction2 = lm(medv ~ . + (black * lstat), data = Boston)
summary(mod_linear)$sigma
summary(Mod_interaction2)$sigma
```

Fit a model using all possible first-order interactions. Verify it is "better" than the linear model. Do you think you overfit? Why or why not?

```{r}
final_interactions = lm(medv ~ .*., data = Boston)
summary(mod_linear)$sigma
summary(final_interactions)$sigma
```

# CV

Use 5-fold CV to estimate the generalization error of the model with all interactions.

```{r}
pacman::p_load(caret)
CV = trainControl(method = "cv", number = 5)
Mod <- train(medv ~ .*., data = Boston, method = "lm",
               trControl = CV)
print(mod)
```





